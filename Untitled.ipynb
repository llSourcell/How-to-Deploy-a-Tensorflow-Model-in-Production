{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Deploy a Tensorflow Model in Production\n",
    "\n",
    "We know how to \n",
    "- write models\n",
    "- train models\n",
    "- test models\n",
    "\n",
    "but how do we deploy them for production use?\n",
    "\n",
    "Let's create a simple  webapp that will allow the user to upload an image and run the Inception model over it for classifying.\n",
    "\n",
    "##  Tensorflow Serving \n",
    "\n",
    "![Image of Yaktocat](https://cdn-images-1.medium.com/max/1800/0*O7yprjYDk2WTO3__.)\n",
    "\n",
    "![Image of Yaktocat](https://blogs.nvidia.com/wp-content/uploads/2016/08/ai_difference_between_deep_learning_training_inference.jpg)\n",
    "\n",
    "- Google's open source library that accompanies Tensorflow\n",
    "- Meant for Inference. (managing models, giving versioned access via reference-counted lookup table i.e HTTP interface via RPC) \n",
    "https://apihandyman.io/do-you-really-know-why-you-prefer-rest-over-rpc/\n",
    "- Can serve multiple models simultaneously (great for A/B testing)\n",
    "- Can serve multiple versions of the same model\n",
    "- written in C++ \n",
    "\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "![Image of Yaktocat](https://tensorflow.github.io/serving/images/serving_architecture.svg)\n",
    "\n",
    "### 4 major components\n",
    "\n",
    "### Servables\n",
    "\n",
    "- The central abstraction in TensorFlow Serving. They are the objects that clients use to perform computation (for example, a lookup or inference).\n",
    "- Flexible size (single lookup table shard, model, multiple models)\n",
    "- Good for Concurrent operations, A/B testing\n",
    "- Multiple versions of a servable in one instance helps refresh configs\n",
    "- Streams are sorted in-order like Git\n",
    "\n",
    "### Loaders\n",
    "\n",
    "- manage a servable's life cycle. Enables common infrastructure, standardizes the APIs for loading and unloading a servable.\n",
    "\n",
    "### Sources\n",
    "\n",
    "- plugin modules that originates zero or more servable streams. For each stream, a Source supplies one Loader instance for each version it wants to have loaded. \n",
    "\n",
    "### Managers\n",
    "\n",
    "- handle the full lifecycle of Servables (loading, serving, unloading)\n",
    "- listen to Sources and track all versions. \n",
    "- tries to fulfill Sources' requests, but may refuse to load an aspired version if, say, required resources aren't available. \n",
    "- may wait to unload until a newer version finishes loading, based on a policy to guarantee that at least one version is loaded at all times.\n",
    "\n",
    "### 2 Step Process \n",
    "\n",
    "1. Sources create Loaders for Servable Versions.\n",
    "2. Loaders are sent as Aspired Versions to the Manager, which loads and serves them to client requests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Setup Development Environment\n",
    "\n",
    "Manually install from source? No. Let's use Docker.\n",
    "\n",
    "\n",
    "Docker is like a lightweight version of a virtual machine image that runs without the overhead of running a full OS inside it. It's like an app-specific VM. No need to worry about conflicting versions and other entanglements with the rest of the OS. \n",
    "\n",
    "![Image of Yaktocat](http://zdnet2.cbsistatic.com/hub/i/r/2017/05/08/af178c5a-64dd-4900-8447-3abd739757e3/resize/770xauto/78abd09a8d41c182a28118ac0465c914/docker-vm-container.png)\n",
    "\n",
    "Install Instructions https://docs.docker.com/engine/installation/.\n",
    "\n",
    "Let's first clone the Tensorflow serving repo\n",
    "\n",
    "```\n",
    "git clone --recursive https://github.com/tensorflow/serving\n",
    "cd serving\n",
    "```\n",
    "\n",
    "Now we can create a docker image with all the required dependencies(pip dependencies, bazel, grpc)\n",
    "\n",
    "```\n",
    "docker build --pull -t $USER/tensorflow-serving-devel -f tensorflow_serving/tools/docker/Dockerfile.devel .\n",
    "```\n",
    "\n",
    "\n",
    "Now let's run the container locally. Once it's running it will let us work in a terminal inside of it. \n",
    "\n",
    "```\n",
    "docker run --name=tensorflow_container -it $USER/tensorflow-serving-devel\n",
    "```\n",
    "\n",
    "Now we can clone Tensorflow serving into our dependency-ready container\n",
    "\n",
    "git clone --recursive https://github.com/tensorflow/serving\n",
    "cd serving/tensorflow\n",
    "./configure\n",
    "\n",
    "\n",
    "Now we need to build it using Google's Bazel build tool from inside our container.  Bazel manages third party dependencies at code level, downloading and building them, as long as they are also built with Bazel. \n",
    "\n",
    "Dependencies needed are\n",
    "\n",
    "* tensorflow serving\n",
    "* pre-trained inception model\n",
    "\n",
    "\n",
    "First TF Serving (This will take like 20-50 minutes)\n",
    "\n",
    "```\n",
    "cd ..\n",
    "bazel build -c opt tensorflow_serving/...\n",
    "```\n",
    "\n",
    "Once completed we can test it out by running the model server\n",
    "\n",
    "```\n",
    "bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server\n",
    "```\n",
    "\n",
    "Output should look like this if install was successful\n",
    "\n",
    "```\n",
    "Usage: model_server [--port=8500] [--enable_batching] [--model_name=my_name] --model_base_path=/path/to/export\n",
    "```\n",
    "\n",
    "Now for dependency 2 of 2, the Inception Model. It's a Deep convolutional neural network that achieved state of the art classification in the ImageNet competition in 2014. Trained on hundreds of thousands of images.\n",
    "\n",
    "```\n",
    "curl -O http://download.tensorflow.org/models/image/imagenet/inception-v3-2016-03-01.tar.gz\n",
    "tar xzf inception-v3-2016-03-01.tar.gz\n",
    "bazel-bin/tensorflow_serving/example/inception_export --checkpoint_dir=inception-v3 --export_dir=inception-export\n",
    "```\n",
    "![Image of Yaktocat](https://1.bp.blogspot.com/-O7AznVGY9js/V8cV_wKKsMI/AAAAAAAABKQ/maO7n2w3dT4Pkcmk7wgGqiSX5FUW2sfZgCLcB/s1600/image00.png)\n",
    "\n",
    "Let's run it and the gRPC server locally!\n",
    "\n",
    "\n",
    "```\n",
    "bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=inception --model_base_path=inception-export &> inception_log &\n",
    "```\n",
    "\n",
    "\n",
    "Now that it's running on our local server, let's test it out using our python client app. We'll query it using a panda picture and it'll return a classification output\n",
    "\n",
    "```\n",
    "wget https://upload.wikimedia.org/wikipedia/en/a/ac/Xiang_Xiang_panda.jpg\n",
    "bazel-bin/tensorflow_serving/example/inception_client --server=localhost:9000 --image=./Xiang_Xiang_panda.jpg\n",
    "```\n",
    "\n",
    "If everything works, we'll see a panda classification output to terminal!\n",
    "\n",
    "Wanna push this to the cloud? Well using Google cloud and the automatic container management tool (https://kubernetes.io/) we can. See part 2 of this tutorial to do that \n",
    "\n",
    "https://tensorflow.github.io/serving/serving_inception\n",
    "\n",
    "and when we're ready to build our own model\n",
    "\n",
    "https://tensorflow.github.io/serving/serving_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
